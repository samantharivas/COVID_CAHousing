CA_ts <- ts(home_prices_imp$CA, frequency = 12)
seasonal_components <- stl(CA_ts, s.window = "periodic")
# deseasonalize the data
home_prices_imp$deseasonalized <- seasonal_components$time.series[, "remainder"]
home_prices_imp
# Plot the decomposed components
plot(decomposed$seasonal, main = "Seasonal Component")
plot(decomposed$trend, main = "Trend")
# perform Augmented Dickey-Fuller test
library(tseries)
# convert the deseasonalized column to a numeric vector
home_prices_imp$deseasonalized <- as.numeric(home_prices_imp$deseasonalized)
adf_test_result <- adf.test(home_prices_imp$deseasonalized)
adf_test_result
# check p-value for stationarity
if (adf_test_result$p.value < 0.05) {
cat("Time series is stationary.\n")
} else {
cat("Time series is not stationary. Applying differencing.\n")
}
# apply differencing to the deseasonalized component
home_prices_imp$deseasonalized_diff <- c(NA, diff(home_prices_imp$deseasonalized))
home_prices_imp
ACF <- acf(home_prices_imp$CA, lag.max = 50)
pacf <- pacf(home_prices_imp$CA, lag.max = 50)
#simple moving average calculation
home_prices_imp$MA <- zoo::rollapply(home_prices_imp$CA, width = 4, FUN = mean, fill = NA)
ggplot(home_prices_imp, aes(x = Year)) +
geom_line(aes(y = CA, color = "Actual Sales")) +
geom_line(aes(y = MA, color = "MA")) +
labs(title = "", x = "Year", y = "")+
scale_color_manual(values = c("Actual Sales" = "blue", "MA" = "red")) +
theme_minimal()
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
# rename df
CA <- home_prices_imp
CA
# create a ts for CA column
CA_ts <- ts(CA$CA, frequency = 12, start = c(1990, 1), end = c(2023, 9))
# create additional lag features
CA$lag1 <- lag(CA$deseasonalized_diff, 1)
CA$lag2 <- lag(CA$deseasonalized_diff, 2)
CA$lag3 <- lag(CA$deseasonalized_diff, 3)
# rolling statistics
CA$rolling_std <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = sd, fill = NA, align = "right")
CA$rolling_mean <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = mean, fill = NA, align = "right")
# calculate correlation matrix
cor_matrix <- cor(CA[, c("CA", "lag1", "lag2", "lag3", "rolling_std", "rolling_mean")], use = "complete.obs")
print(cor_matrix)
# apply log transformation to the target variable - data transformation
CA$CA_log <- log(CA$CA + 1)
# Create 2 Partitions
# Training set is from January 1990 - December 2019
# Validation set is from January 2020 - September 2023
train <- window(CA_ts, start = c(1990, 1), end = c(2019, 12))
validation <- window(CA_ts, start = c(2020, 1))
autoplot(train) +
autolayer(validation, color = "blue")
# fit linear model
home_prices.lm <- tslm(CA_ts ~ trend + I(trend^2))
# plot linear model
plot(CA_ts, xlab = "Time", ylab = "Shipments", ylim = c(160000 , 900000), bty = "l")
lines(home_prices.lm$fitted, lwd = 2)
CA_ts_zoom <- window(CA_ts, start = c(1990, 1), end = c(2023, 9))
plot(CA_ts_zoom, xlab = "Year", ylab = "CA", ylim = c(160000 , 900000), bty = "l")
# Naive Model
n_model <- naive(train, h = 5, level = 95)
# plot the Naive Model - Training set
autoplot(train) +
autolayer(n_model, color = "red", series = "Naive") +
autolayer(validation, color = "blue", series = "Actual")
# Seasonal Naive Model
sn_model <- snaive(train, h = 5)
# plot the naive Model - Training set
autoplot(train) +
autolayer(sn_model, color = "red", series = "Seasonal Naive") +
autolayer(validation, color = "blue", series = "Actual")
# Season and Trend
st_model <- tslm(train ~ trend + season)
summary(st_model)
st_forecast <- forecast(st_model, h=5)
st_forecast
# plot the Season and Trend Model
autoplot(train) +
autolayer(st_forecast, color = "red", series = "season trend") +
autolayer(validation, color = "blue", series = "Actual")
# Determining the Error
accuracy(st_forecast)
accuracy(n_model)
accuracy(sn_model)
mean(abs(n_model$residuals), na.rm = T)
sqrt(mean(n_model$residuals^2, na.rm = T))
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
# Create a correlation heatmap
heatmap(correlation_matrix,
col = colorRampPalette(c("blue", "white", "red"))(100),
main = "Correlation Heatmap",
cexRow = 1, cexCol = 1, margins = c(10, 10))
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
# Create a correlation heatmap
heatmap(correlation_matrix,
main = "Correlation Heatmap",
cexRow = 1, cexCol = 1)
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
# import necessary libaries
library(tidyverse)
library(fpp3)
library(gridExtra)
library(ggplot2)
library(tsibble)
library(forecast)
library(patchwork)
library(fable)
library(readxl)
library(moments)
library(imputeTS)
library(zoo)
library(TTR)
# import excel file
home_prices <- read_excel("MedianPricesofExistingDetachedHomesHistoricalData.xlsx", skip = 6) |>
mutate(`Mon-Yr` = as.Date(`Mon-Yr`, format = "%Y-%m")) |>
as_tsibble(index = "Mon-Yr") # create tsibble
# plot ts object
autoplot(home_prices, CA) +
theme_minimal()
# check dimension of df
dim(home_prices) # df holds 405 rows and 65 columns
home_prices
str(home_prices) # df consists of numeric and character values
# convert columns to correct data type
# specify the columns labeled as character to convert to numeric
cols_to_convert <- c("Amador", "Calaveras", "Contra-Costa", "Del Norte", "El Dorado", "Glenn", "Kings", "Lake",
"Lassen", "Madera", "Mariposa", "Nevada", "Plumas", "San Benito", "San Mateo",
"Shasta", "Siskiyou", "Solano", "Stanislaus", "Sutter", "Tehama", "Trinity",
"Tuolumne", "Monterey", "Mono", "Santa Cruz", "San Luis Obispo",
"San Benito", "San Mateo", "Shasta", "Siskiyou", "Solano", "Sonoma", "Stanislaus",
"Sutter", "Tehama", "Trinity", "Tulare", "Ventura", "Yolo", "Yuba", "Condo",
"LA Metro", "Central Coast", "Central Valley", "Far North", "Inland Empire",
"S.F. Bay Area", "SoCal", "Mendocino", "San Joaquin")
# convert listed columns to numeric
for (col in cols_to_convert) {
home_prices[[col]] <- as.numeric(as.character(home_prices[[col]]))
}
# check the structure of your data frame after conversion
str(home_prices)
# convert date to MM/YYYY format
home_prices$date <- as.Date(home_prices$`Mon-Yr`, format = "%Y-%m-%d")
home_prices$date <- as.Date(home_prices$date, format ="%m %Y")
# create seperate column for month and year
home_prices <- home_prices %>%
mutate(Month = month(date), Year = year(date))
# remove mon-yr column and ...55 column
columns_to_keep <- setdiff(names(home_prices), c("Mon-Yr", "...55"))
home_prices <- home_prices[, columns_to_keep]
#view the updated data frame
print(home_prices)
# make a subset df removing condo, LA metro, Central Coast, Central Valley, Far North, Inland Empire, and S.F. Bay Area
regions_to_exclude <- c("Condo", "LA Metro", "Central Coast", "Central Valley", "Far North", "Inland Empire", "S.F. Bay Area", "SoCal")
home_prices <- home_prices[, !(colnames(home_prices) %in% regions_to_exclude)]
#check for duplicates
sum(duplicated(home_prices)) # no duplicate values are present
#identify outliers using boxplot
par(mfrow = c(2, 4))
# Exclude the "Mon-Yr" column
numeric_columns <- names(home_prices)[sapply(home_prices, is.numeric) & names(home_prices) != "Mon-Yr"]
for (col in numeric_columns) {
boxplot(home_prices[[col]],
xlab = col,
main = paste(col, "Boxplot"))
}
for (col in numeric_columns) {
hist(home_prices[[col]],
xlab = col,
main = paste(col, "Histogram"))
}
# check for missing values
missing_values <- colSums(is.na(home_prices))
print(missing_values)
# check missing percentage of values per column
missing_percentage <- colMeans(is.na(home_prices)) * 100
missing_percentage <- missing_percentage[order(-missing_percentage)]
missing_percentage
# identify columns with more than 50% missing values
columns_to_drop <- names(missing_percentage[missing_percentage > 15])
# print columns with more than 15% missing values
cat("Columns with More than 15% Missing Values:\n")
cat(columns_to_drop, sep = ", ")
# drop the identified columns with more than 50% missing values from the DataFrame
home_prices <- home_prices[, !(names(home_prices) %in% columns_to_drop)]
# use imputer to fill missing value with mean
home_prices_imp <- home_prices %>%
mutate(across(-date, ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))
# exclude non-numeric columns
numeric_columns_imp <- home_prices_imp[sapply(home_prices_imp, is.numeric)]
# calculate skewness for all numeric imputed variables
skewness_values_imp <- moments::skewness(numeric_columns_imp, na.rm = TRUE)
print(skewness_values_imp)
# calculate skewness of original df
# exclude non-numeric columns
numeric_columns <- home_prices[sapply(home_prices, is.numeric)]
# calculate skewness for all numeric variables
skewness_values <- moments::skewness(numeric_columns, na.rm = TRUE)
print(skewness_values)
# compare skewness before and after
skewness_all <- data.frame(
Original_Skewness = skewness_values,
Skewness_Imputed= skewness_values_imp
)
skewness_all
summary(home_prices_imp)
dim(home_prices_imp) # new df has 405 rows and 34 columns
str(home_prices_imp)
# decompose ts
# decompose the time series
decomposed <- decompose(ts(home_prices_imp$CA, frequency = 12))
autoplot(decomposed) +
labs(title = "Seasonal Decomposition")
plot(home_prices_imp$CA, type = "l", main = "Time Series Plot")
# perform seasonal decomposition using STL
# convert df into ts
ts_data <- ts(home_prices_imp, frequency = 12)
CA_ts <- ts(home_prices_imp$CA, frequency = 12)
seasonal_components <- stl(CA_ts, s.window = "periodic")
# deseasonalize the data
home_prices_imp$deseasonalized <- seasonal_components$time.series[, "remainder"]
home_prices_imp
# Plot the decomposed components
plot(decomposed$seasonal, main = "Seasonal Component")
plot(decomposed$trend, main = "Trend")
# perform Augmented Dickey-Fuller test
library(tseries)
# convert the deseasonalized column to a numeric vector
home_prices_imp$deseasonalized <- as.numeric(home_prices_imp$deseasonalized)
adf_test_result <- adf.test(home_prices_imp$deseasonalized)
adf_test_result
# check p-value for stationarity
if (adf_test_result$p.value < 0.05) {
cat("Time series is stationary.\n")
} else {
cat("Time series is not stationary. Applying differencing.\n")
}
# apply differencing to the deseasonalized component
home_prices_imp$deseasonalized_diff <- c(NA, diff(home_prices_imp$deseasonalized))
home_prices_imp
ACF <- acf(home_prices_imp$CA, lag.max = 50)
pacf <- pacf(home_prices_imp$CA, lag.max = 50)
# remove missing values
clean_diff<- na.omit(home_prices_imp$deseasonalized_diff)
# calculate ACF/PACF on cleaned df
ACF_diff <- acf(clean_diff, lag.max = 50)
pacf_diff <- pacf(clean_diff, lag.max = 50)
#simple moving average calculation
home_prices_imp$MA <- zoo::rollapply(home_prices_imp$CA, width = 4, FUN = mean, fill = NA, align = "right")
ggplot(home_prices_imp, aes(x = date, y = CA)) +
geom_line(aes(y = CA, color = "Actual Sales")) +
geom_line(aes(y = MA, color = "MA")) +
labs(title = "", x = "Year", y = "")+
scale_color_manual(values = c("Actual Sales" = "blue", "MA" = "red")) +
theme_minimal()
home_prices_imp
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
# rename df
CA <- home_prices_imp
CA
# create a ts for CA column
CA_ts <- ts(CA$CA, frequency = 12, start = c(1990, 1), end = c(2023, 9))
# create additional lag features
CA$lag1 <- lag(CA$deseasonalized_diff, 1)
CA$lag2 <- lag(CA$deseasonalized_diff, 2)
CA$lag3 <- lag(CA$deseasonalized_diff, 3)
# rolling statistics
CA$rolling_std <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = sd, fill = NA, align = "right")
CA$rolling_mean <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = mean, fill = NA, align = "right")
# calculate correlation matrix
cor_matrix <- cor(CA[, c("CA", "lag1", "lag2", "lag3", "rolling_std", "rolling_mean")], use = "complete.obs")
print(cor_matrix)
# apply log transformation to the target variable - data transformation
CA$CA_log <- log(CA$CA + 1)
# Create 2 Partitions
# Training set is from January 2014- December 2018
# Validation set is from January 2019 - September 2023
train <- window(CA_ts, start = c(2014, 1), end = c(2018, 12))
validation <- window(CA_ts, start = c(2019, 1))
autoplot(train) +
autolayer(validation, color = "blue")
n_model <- naive(train, h = 57)
# plot the Naive Model - Training set
autoplot(train) +
autolayer(n_model, color = "red", series = "Naive") +
autolayer(validation, color = "blue", series = "Actual")
summary(n_model)
sn_model <- snaive(train, h = 57)
# plot the naive Model - Training set
autoplot(train) +
autolayer(sn_model, color = "red", series = "Seasonal Naive") +
autolayer(validation, color = "blue", series = "Actual")
st_model <- tslm(train ~ trend + season)
summary(st_model)
st_forecast <- forecast(st_model, h=57)
st_forecast
# plot the Season and Trend Model
autoplot(train) +
autolayer(st_forecast, color = "red", series = "season trend") +
autolayer(validation, color = "blue", series = "Actual")
# Determining the Error
accuracy(n_model)
accuracy(sn_model)
accuracy(st_forecast)
# SARIMA model
sarima_model <- auto.arima(train)
sarima_forecast <- forecast(sarima_model, h = 57)
# plot SARIMA model
autoplot(train) +
autolayer(sarima_forecast, color = "green", series = "SARIMA Forecast") +
autolayer(validation, color = "blue", series = "Actual")
# Fit the Theta model
theta_model <- thetaf(train)
# Generate forecasts
theta_forecast <- forecast(theta_model, h = 5)
# Plot the forecast
autoplot(theta_forecast)
# Fit the TBATS model
tbats_model <- tbats(train)
# Generate forecasts
tbats_forecast <- forecast(tbats_model, h = 57)
# Plot the forecast
autoplot(tbats_forecast) +
autolayer(validation, color = "blue", series = "Actual")
# Fit an ARIMA model using the training set
arima_model <- auto.arima(train)
# Print the model summary
print(summary(arima_model))
# Forecast future values using the ARIMA model on the validation set
arima_forecast <- forecast(arima_model, h = length(validation))
# Plot the original time series, fitted values, and forecast
autoplot(CA_ts) +
autolayer(fitted(arima_model), series = "Fitted", color = "blue") +
autolayer(arima_forecast, series = "Forecast", color = "red") +
labs(title = "ARIMA Model Fitting and Forecasting", y = "CA Sales") +
theme_minimal()
# Evaluate the forecast accuracy on the validation set
accuracy(arima_forecast, validation)
# Print the forecast values
print(arima_forecast)
# Fit an ETS model using the univariate time series
ets_model <- ets(train)
# Print the model summary
print(summary(ets_model))
# Forecast future values using the ETS model on the validation set
ets_forecast <- forecast(ets_model, h = length(validation))
# Plot the original time series, fitted values, and forecast
autoplot(CA_ts) +
autolayer(fitted(ets_model), series = "Fitted", color = "blue") +
autolayer(ets_forecast, series = "Forecast", color = "red") +
labs(title = "ETS Model Fitting and Forecasting", y = "CA Sales") +
theme_minimal()
# Evaluate the forecast accuracy on the validation set
accuracy(ets_forecast, validation)
# Print the forecast values
print(ets_forecast)
# Evaluate accuracy
sarima_accuracy <- accuracy(sarima_forecast)
theta_accuracy <- accuracy(theta_forecast)
tbats_accuracy <- accuracy(tbats_forecast)
naive_accuracy <- accuracy(n_model)
snaive_accuracy <- accuracy(sn_model)
st_accuracy <- accuracy(st_forecast)
arima_accuracy <- accuracy(arima_forecast)
ets_accuracy <- accuracy(ets_forecast)
# Create a table
accuracy_table <- data.frame(
Model = c("SARIMA", "Theta", "TBATS", "Naive", "Seasonal_Naive", "Trend_Season", "ARIMA", "ETS"),
RMSE = c(sarima_accuracy[1], theta_accuracy[1], tbats_accuracy[1], naive_accuracy[1], snaive_accuracy[1], st_accuracy[1],
arima_accuracy[1], ets_accuracy[1]),
MAE = c(sarima_accuracy[2], theta_accuracy[2], tbats_accuracy[2], naive_accuracy[2], snaive_accuracy[2], st_accuracy[2],
arima_accuracy[2], ets_accuracy[2]),
MAPE = c(sarima_accuracy[3], theta_accuracy[3], tbats_accuracy[3], naive_accuracy[3], snaive_accuracy[3], st_accuracy[3],
arima_accuracy[3], ets_accuracy[3])
)
# Print the table
print(accuracy_table)
# create a ts for Riverside County
riv_ts <- ts(CA$Riverside, frequency = 12, start = c(1990, 1), end = c(2023, 9))
# create training and validation set
train_riv <- window(riv_ts, start = c(2014, 1), end = c(2018, 12))
validation_riv <- window(riv_ts, start = c(2019, 1))
# fit SARIMA model on Riverside ts
riv_sarima <- auto.arima(train_riv)
# print the model summary
print(summary(riv_sarima))
# forecast future values using the SARIMA model on the validation set - Riverside
riv_forecast <- forecast(riv_sarima, h = length(validation))
# plot the original time series, fitted values, and forecast
autoplot(riv_ts) +
autolayer(fitted(riv_sarima), series = "Fitted", color = "blue") +
autolayer(riv_forecast, series = "Forecast", color = "red") +
labs(title = "SARIMA Model Fitting and Forecasting - Riverside", y = "Riverside Sales") +
theme_minimal()
# Evaluate the forecast accuracy on the validation set
accuracy(riv_forecast, validation)
# Print the forecast values
print(riv_forecast)
# create a ts for Sacramento County
sac_ts <- ts(CA$Sacramento, frequency = 12, start = c(1990, 1), end = c(2023, 9))
# create training and validation set
train_sac <- window(sac_ts, start = c(2014, 1), end = c(2018, 12))
validation_sac <- window(sac_ts, start = c(2019, 1))
# fit SARIMA model on Sacramento ts
sac_sarima <- auto.arima(train_sac)
# print the model summary
print(summary(sac_sarima))
# forecast future values using the SARIMA model on the validation set - Sacramento
sac_forecast <- forecast(sac_sarima, h = length(validation))
# plot the original time series, fitted values, and forecast
autoplot(sac_ts) +
autolayer(fitted(sac_sarima), series = "Fitted", color = "blue") +
autolayer(sac_forecast, series = "Forecast", color = "red") +
labs(title = "SARIMA Model Fitting and Forecasting - Sacramento", y = "Sacramento Sales") +
theme_minimal()
# Evaluate the forecast accuracy on the validation set
accuracy(sac_forecast, validation)
# Print the forecast values
print(sac_forecast)
# create a ts for Alameda County
al_ts <- ts(CA$Alameda, frequency = 12, start = c(1990, 1), end = c(2023, 9))
# create training and validation set
train_al <- window(al_ts, start = c(2014, 1), end = c(2018, 12))
validation_al <- window(al_ts, start = c(2019, 1))
# fit SARIMA model on Alameda ts
al_sarima <- auto.arima(train_al)
# print the model summary
print(summary(al_sarima))
# forecast future values using the SARIMA model on the validation set - Alameda
al_forecast <- forecast(al_sarima, h = length(validation))
# plot the original time series, fitted values, and forecast
autoplot(al_ts) +
autolayer(fitted(al_sarima), series = "Fitted", color = "blue") +
autolayer(al_forecast, series = "Forecast", color = "red") +
labs(title = "SARIMA Model Fitting and Forecasting - Alameda", y = "Alameda Sales") +
theme_minimal()
# Evaluate the forecast accuracy on the validation set
accuracy(al_forecast, validation)
# Print the forecast values
print(al_forecast)
# create a ts for Santa Clara  County
sc_ts <- ts(CA$`Santa Clara`, frequency = 12, start = c(1990, 1), end = c(2023, 9))
# create training and validation set
train_sc <- window(sc_ts, start = c(2014, 1), end = c(2018, 12))
validation_sc <- window(sc_ts, start = c(2019, 1))
# fit SARIMA model on Alameda ts
sc_sarima <- auto.arima(train_sc)
# print the model summary
print(summary(sc_sarima))
# forecast future values using the SARIMA model on the validation set - Santa Clara
sc_forecast <- forecast(sc_sarima, h = length(validation))
# plot the original time series, fitted values, and forecast
autoplot(sc_ts) +
autolayer(fitted(sc_sarima), series = "Fitted", color = "blue") +
autolayer(sc_forecast, series = "Forecast", color = "red") +
labs(title = "SARIMA Model Fitting and Forecasting - Santa Clara", y = "Santa Clara Sales") +
theme_minimal()
# Evaluate the forecast accuracy on the validation set
accuracy(sc_forecast, validation)
# Print the forecast values
print(sc_forecast)
# create a ts for Los Angeles County
la_ts <- ts(CA$Alameda, frequency = 12, start = c(1990, 1), end = c(2023, 9))
# create training and validation set
train_la <- window(la_ts, start = c(2014, 1), end = c(2018, 12))
validation_la <- window(la_ts, start = c(2019, 1))
# fit SARIMA model on Los Angeles ts
la_sarima <- auto.arima(train_la)
# print the model summary
print(summary(la_sarima))
# forecast future values using the SARIMA model on the validation set - Los Angeles
la_forecast <- forecast(la_sarima, h = length(validation))
# plot the original time series, fitted values, and forecast
autoplot(la_ts) +
autolayer(fitted(la_sarima), series = "Fitted", color = "blue") +
autolayer(la_forecast, series = "Forecast", color = "red") +
labs(title = "SARIMA Model Fitting and Forecasting - Los Angeles", y = "Los Amgeles Sales") +
theme_minimal()
# Evaluate the forecast accuracy on the validation set
accuracy(la_forecast, validation)
# Print the forecast values
print(la_forecast)
# create a ts for San Diego County
sd_ts <- ts(CA$`San Diego`, frequency = 12, start = c(1990, 1), end = c(2023, 9))
# create training and validation set
train_sd <- window(sd_ts, start = c(2014, 1), end = c(2018, 12))
validation_sd <- window(sd_ts, start = c(2019, 1))
# fit SARIMA model on San Diego ts
sd_sarima <- auto.arima(train_sd)
# print the model summary
print(summary(sd_sarima))
# forecast future values using the SARIMA model on the validation set - San Diego
sd_forecast <- forecast(sd_sarima, h = length(validation))
# plot the original time series, fitted values, and forecast
autoplot(sd_ts) +
autolayer(fitted(sd_sarima), series = "Fitted", color = "blue") +
autolayer(sd_forecast, series = "Forecast", color = "red") +
labs(title = "SARIMA Model Fitting and Forecasting - San Diego", y = "San Diego Sales") +
theme_minimal()
# Evaluate the forecast accuracy on the validation set
accuracy(sd_forecast, validation)
# Print the forecast values
print(sd_forecast)
