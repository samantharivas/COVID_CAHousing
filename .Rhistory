# import necessary libaries
library(tidyverse)
library(fpp3)
library(gridExtra)
library(ggplot2)
library(tsibble)
library(forecast)
library(patchwork)
library(fable)
library(readxl)
library(moments)
library(imputeTS)
library(zoo)
library(TTR)
# import excel file
home_prices <- read_excel("MedianPricesofExistingDetachedHomesHistoricalData.xlsx", skip = 6) |>
mutate(`Mon-Yr` = as.Date(`Mon-Yr`, format = "%Y-%m")) |>
as_tsibble(index = "Mon-Yr") # create tsibble
# plot ts object
autoplot(home_prices, CA) +
theme_minimal()
# check dimension of df
dim(home_prices) # df holds 405 rows and 65 columns
home_prices
str(home_prices) # df consists of numeric and character values
# convert columns to correct data type
# specify the columns labeled as character to convert to numeric
cols_to_convert <- c("Amador", "Calaveras", "Contra-Costa", "Del Norte", "El Dorado", "Glenn", "Kings", "Lake",
"Lassen", "Madera", "Mariposa", "Nevada", "Plumas", "San Benito", "San Mateo",
"Shasta", "Siskiyou", "Solano", "Stanislaus", "Sutter", "Tehama", "Trinity",
"Tuolumne", "Monterey", "Mono", "Santa Cruz", "San Luis Obispo",
"San Benito", "San Mateo", "Shasta", "Siskiyou", "Solano", "Sonoma", "Stanislaus",
"Sutter", "Tehama", "Trinity", "Tulare", "Ventura", "Yolo", "Yuba", "Condo",
"LA Metro", "Central Coast", "Central Valley", "Far North", "Inland Empire",
"S.F. Bay Area", "SoCal", "Mendocino", "San Joaquin")
# convert listed columns to numeric
for (col in cols_to_convert) {
home_prices[[col]] <- as.numeric(as.character(home_prices[[col]]))
}
# check the structure of your data frame after conversion
str(home_prices)
# convert date to MM/YYYY format
home_prices$date <- as.Date(home_prices$`Mon-Yr`, format = "%Y-%m-%d")
home_prices$date <- as.Date(home_prices$date, format ="%m %Y")
# create seperate column for month and year
home_prices <- home_prices %>%
mutate(Month = month(date), Year = year(date))
# remove mon-yr column and ...55 column
columns_to_keep <- setdiff(names(home_prices), c("Mon-Yr", "...55"))
home_prices <- home_prices[, columns_to_keep]
#view the updated data frame
print(home_prices)
# make a subset df removing condo, LA metro, Central Coast, Central Valley, Far North, Inland Empire, and S.F. Bay Area
regions_to_exclude <- c("Condo", "LA Metro", "Central Coast", "Central Valley", "Far North", "Inland Empire", "S.F. Bay Area", "SoCal")
home_prices <- home_prices[, !(colnames(home_prices) %in% regions_to_exclude)]
#check for duplicates
sum(duplicated(home_prices)) # no duplicate values are present
#identify outliers using boxplot
par(mfrow = c(2, 4))
# Exclude the "Mon-Yr" column
numeric_columns <- names(home_prices)[sapply(home_prices, is.numeric) & names(home_prices) != "Mon-Yr"]
for (col in numeric_columns) {
boxplot(home_prices[[col]],
xlab = col,
main = paste(col, "Boxplot"))
}
for (col in numeric_columns) {
hist(home_prices[[col]],
xlab = col,
main = paste(col, "Histogram"))
}
# check for missing values
missing_values <- colSums(is.na(home_prices))
print(missing_values)
# check missing percentage of values per column
missing_percentage <- colMeans(is.na(home_prices)) * 100
missing_percentage <- missing_percentage[order(-missing_percentage)]
missing_percentage
# identify columns with more than 50% missing values
columns_to_drop <- names(missing_percentage[missing_percentage > 15])
# print columns with more than 15% missing values
cat("Columns with More than 15% Missing Values:\n")
cat(columns_to_drop, sep = ", ")
# drop the identified columns with more than 50% missing values from the DataFrame
home_prices <- home_prices[, !(names(home_prices) %in% columns_to_drop)]
# use imputer to fill missing value with median
home_prices_imp <- home_prices %>%
mutate(across(-date, ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))
# exclude non-numeric columns
numeric_columns_imp <- home_prices_imp[sapply(home_prices_imp, is.numeric)]
# calculate skewness for all numeric imputed variables
skewness_values_imp <- moments::skewness(numeric_columns_imp, na.rm = TRUE)
print(skewness_values_imp)
# calculate skewness of original df
# exclude non-numeric columns
numeric_columns <- home_prices[sapply(home_prices, is.numeric)]
# calculate skewness for all numeric variables
skewness_values <- moments::skewness(numeric_columns, na.rm = TRUE)
print(skewness_values)
# compare skewness before and after
skewness_all <- data.frame(
Original_Skewness = skewness_values,
Skewness_Imputed= skewness_values_imp
)
skewness_all
summary(home_prices_imp)
dim(home_prices_imp) # new df has 405 rows and 34 columns
str(home_prices_imp)
# decompose ts
# decompose the time series
decomposed <- decompose(ts(home_prices_imp$CA, frequency = 12))
autoplot(decomposed) +
labs(title = "Seasonal Decomposition")
plot(home_prices_imp$CA, type = "l", main = "Time Series Plot")
# perform seasonal decomposition using STL
# convert df into ts
ts_data <- ts(home_prices_imp, frequency = 12)
CA_ts <- ts(home_prices_imp$CA, frequency = 12)
seasonal_components <- stl(CA_ts, s.window = "periodic")
# deseasonalize the data
home_prices_imp$deseasonalized <- seasonal_components$time.series[, "remainder"]
home_prices_imp
# Plot the decomposed components
plot(decomposed$seasonal, main = "Seasonal Component")
plot(decomposed$trend, main = "Trend")
# perform Augmented Dickey-Fuller test
library(tseries)
# convert the deseasonalized column to a numeric vector
home_prices_imp$deseasonalized <- as.numeric(home_prices_imp$deseasonalized)
adf_test_result <- adf.test(home_prices_imp$deseasonalized)
adf_test_result
# check p-value for stationarity
if (adf_test_result$p.value < 0.05) {
cat("Time series is stationary.\n")
} else {
cat("Time series is not stationary. Applying differencing.\n")
}
# apply differencing to the deseasonalized component
home_prices_imp$deseasonalized_diff <- c(NA, diff(home_prices_imp$deseasonalized))
home_prices_imp
ACF <- acf(home_prices_imp$CA, lag.max = 50)
pacf <- pacf(home_prices_imp$CA, lag.max = 50)
#simple moving average calculation
home_prices_imp$MA <- zoo::rollapply(home_prices_imp$CA, width = 4, FUN = mean, fill = NA)
ggplot(home_prices_imp, aes(x = Year)) +
geom_line(aes(y = CA, color = "Actual Sales")) +
geom_line(aes(y = MA, color = "MA")) +
labs(title = "", x = "Year", y = "")+
scale_color_manual(values = c("Actual Sales" = "blue", "MA" = "red")) +
theme_minimal()
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
# rename df
CA <- home_prices_imp
CA
# create a ts for CA column
CA_ts <- ts(CA$CA, frequency = 12, start = c(1990, 1), end = c(2023, 9))
# create additional lag features
CA$lag1 <- lag(CA$deseasonalized_diff, 1)
CA$lag2 <- lag(CA$deseasonalized_diff, 2)
CA$lag3 <- lag(CA$deseasonalized_diff, 3)
# rolling statistics
CA$rolling_std <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = sd, fill = NA, align = "right")
CA$rolling_mean <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = mean, fill = NA, align = "right")
# calculate correlation matrix
cor_matrix <- cor(CA[, c("CA", "lag1", "lag2", "lag3", "rolling_std", "rolling_mean")], use = "complete.obs")
print(cor_matrix)
# apply log transformation to the target variable - data transformation
CA$CA_log <- log(CA$CA + 1)
# Create 2 Partitions
# Training set is from January 1990 - December 2019
# Validation set is from January 2020 - September 2023
train <- window(CA_ts, start = c(1990, 1), end = c(2019, 12))
validation <- window(CA_ts, start = c(2020, 1))
autoplot(train) +
autolayer(validation, color = "blue")
# fit linear model
home_prices.lm <- tslm(CA_ts ~ trend + I(trend^2))
# plot linear model
plot(CA_ts, xlab = "Time", ylab = "Shipments", ylim = c(160000 , 900000), bty = "l")
lines(home_prices.lm$fitted, lwd = 2)
CA_ts_zoom <- window(CA_ts, start = c(1990, 1), end = c(2023, 9))
plot(CA_ts_zoom, xlab = "Year", ylab = "CA", ylim = c(160000 , 900000), bty = "l")
# Naive Model
n_model <- naive(train, h = 5, level = 95)
# plot the Naive Model - Training set
autoplot(train) +
autolayer(n_model, color = "red", series = "Naive") +
autolayer(validation, color = "blue", series = "Actual")
# Seasonal Naive Model
sn_model <- snaive(train, h = 5)
# plot the naive Model - Training set
autoplot(train) +
autolayer(sn_model, color = "red", series = "Seasonal Naive") +
autolayer(validation, color = "blue", series = "Actual")
# Season and Trend
st_model <- tslm(train ~ trend + season)
summary(st_model)
st_forecast <- forecast(st_model, h=5)
st_forecast
# plot the Season and Trend Model
autoplot(train) +
autolayer(st_forecast, color = "red", series = "season trend") +
autolayer(validation, color = "blue", series = "Actual")
# Determining the Error
accuracy(st_forecast)
accuracy(n_model)
accuracy(sn_model)
mean(abs(n_model$residuals), na.rm = T)
sqrt(mean(n_model$residuals^2, na.rm = T))
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
# Create a correlation heatmap
heatmap(correlation_matrix,
col = colorRampPalette(c("blue", "white", "red"))(100),
main = "Correlation Heatmap",
cexRow = 1, cexCol = 1, margins = c(10, 10))
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
# Create a correlation heatmap
heatmap(correlation_matrix,
main = "Correlation Heatmap",
cexRow = 1, cexCol = 1)
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
