---
title: "COVID_CAHOUSING"
output: pdf_document
date: "2023-11-16"
---

# Effect of COVID on California Housing Prices (1990-2023)

Data Source: <https://www.car.org/marketdata/data/housingdata>.

The data was sources from the California Association of Realtor website.The data contains the median price of existing single-family detached homes in California, ranging from January 1990 until September 2023.

GitHub link: <https://github.com/samantharivas/COVID_CAHousing.git>

## Import Libraries

```{r}
# import necessary libaries 
library(tidyverse)
library(fpp3)
library(gridExtra)
library(ggplot2)
library(tsibble)
library(forecast)
library(patchwork)
library(fable)
library(readxl)
library(moments)
library(imputeTS)
library(zoo)
library(TTR)
```

## Importing Data

```{r}
# import excel file 
home_prices <- read_excel("MedianPricesofExistingDetachedHomesHistoricalData.xlsx", skip = 6) |>
  mutate(`Mon-Yr` = as.Date(`Mon-Yr`, format = "%Y-%m")) |>
  as_tsibble(index = "Mon-Yr") # create tsibble 

# plot ts object 
autoplot(home_prices, CA) + 
  theme_minimal() 
```

## Data Inspection

```{r}
# check dimension of df 
dim(home_prices) # df holds 405 rows and 65 columns 
home_prices
```

```{r}
str(home_prices) # df consists of numeric and character values 
```

```{r}
# convert columns to correct data type 

# specify the columns labeled as character to convert to numeric
cols_to_convert <- c("Amador", "Calaveras", "Contra-Costa", "Del Norte", "El Dorado", "Glenn", "Kings", "Lake", 
                   "Lassen", "Madera", "Mariposa", "Nevada", "Plumas", "San Benito", "San Mateo", 
                   "Shasta", "Siskiyou", "Solano", "Stanislaus", "Sutter", "Tehama", "Trinity", 
                   "Tuolumne", "Monterey", "Mono", "Santa Cruz", "San Luis Obispo", 
                   "San Benito", "San Mateo", "Shasta", "Siskiyou", "Solano", "Sonoma", "Stanislaus", 
                   "Sutter", "Tehama", "Trinity", "Tulare", "Ventura", "Yolo", "Yuba", "Condo", 
                   "LA Metro", "Central Coast", "Central Valley", "Far North", "Inland Empire", 
                   "S.F. Bay Area", "SoCal", "Mendocino", "San Joaquin")

# convert listed columns to numeric
for (col in cols_to_convert) {
  home_prices[[col]] <- as.numeric(as.character(home_prices[[col]]))
}

# check the structure of your data frame after conversion
str(home_prices)

```

```{r}
# convert date to MM/YYYY format
home_prices$date <- as.Date(home_prices$`Mon-Yr`, format = "%Y-%m-%d")
home_prices$date <- as.Date(home_prices$date, format ="%m %Y")

# create seperate column for month and year
home_prices <- home_prices %>%
  mutate(Month = month(date), Year = year(date))

# remove mon-yr column and ...55 column 
columns_to_keep <- setdiff(names(home_prices), c("Mon-Yr", "...55"))
home_prices <- home_prices[, columns_to_keep]

#view the updated data frame
print(home_prices)
```

```{r}
# make a subset df removing condo, LA metro, Central Coast, Central Valley, Far North, Inland Empire, and S.F. Bay Area 
regions_to_exclude <- c("Condo", "LA Metro", "Central Coast", "Central Valley", "Far North", "Inland Empire", "S.F. Bay Area", "SoCal")

home_prices <- home_prices[, !(colnames(home_prices) %in% regions_to_exclude)]
```

```{r}
#check for duplicates
sum(duplicated(home_prices)) # no duplicate values are present 
```

## Exploratory Data Analysis

```{r}
#identify outliers using boxplot
par(mfrow = c(2, 4))

# Exclude the "Mon-Yr" column
numeric_columns <- names(home_prices)[sapply(home_prices, is.numeric) & names(home_prices) != "Mon-Yr"]

for (col in numeric_columns) {
  boxplot(home_prices[[col]],
          xlab = col,
          main = paste(col, "Boxplot"))
}

for (col in numeric_columns) {
  hist(home_prices[[col]],
          xlab = col,
          main = paste(col, "Histogram"))
}

```

```{r}
# check for missing values 
missing_values <- colSums(is.na(home_prices))
print(missing_values)

# check missing percentage of values per column
missing_percentage <- colMeans(is.na(home_prices)) * 100
missing_percentage <- missing_percentage[order(-missing_percentage)]
missing_percentage
```

We decided to drop columns that data was more than 15% missing to maintain data integrity.

```{r}
# identify columns with more than 50% missing values
columns_to_drop <- names(missing_percentage[missing_percentage > 15])

# print columns with more than 15% missing values
cat("Columns with More than 15% Missing Values:\n")
cat(columns_to_drop, sep = ", ")

# drop the identified columns with more than 50% missing values from the DataFrame
home_prices <- home_prices[, !(names(home_prices) %in% columns_to_drop)]

```

```{r}
# use imputer to fill missing value with mean
home_prices_imp <- home_prices %>%
  mutate(across(-date, ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# exclude non-numeric columns 
numeric_columns_imp <- home_prices_imp[sapply(home_prices_imp, is.numeric)] 

# calculate skewness for all numeric imputed variables
skewness_values_imp <- moments::skewness(numeric_columns_imp, na.rm = TRUE)

print(skewness_values_imp)
```

```{r}
# calculate skewness of original df 
# exclude non-numeric columns 
numeric_columns <- home_prices[sapply(home_prices, is.numeric)] 

# calculate skewness for all numeric variables
skewness_values <- moments::skewness(numeric_columns, na.rm = TRUE)

print(skewness_values)
```

```{r}
# compare skewness before and after 
skewness_all <- data.frame(
  Original_Skewness = skewness_values,
  Skewness_Imputed= skewness_values_imp
)

skewness_all
```

```{r}
summary(home_prices_imp)
```

```{r}
dim(home_prices_imp) # new df has 405 rows and 34 columns 
str(home_prices_imp)
```

```{r}
# decompose ts 
# decompose the time series 
decomposed <- decompose(ts(home_prices_imp$CA, frequency = 12))

autoplot(decomposed) +
  labs(title = "Seasonal Decomposition")
```
```{r}
plot(home_prices_imp$CA, type = "l", main = "Time Series Plot")

# perform seasonal decomposition using STL
# convert df into ts 
ts_data <- ts(home_prices_imp, frequency = 12)
CA_ts <- ts(home_prices_imp$CA, frequency = 12)
seasonal_components <- stl(CA_ts, s.window = "periodic")

# deseasonalize the data
home_prices_imp$deseasonalized <- seasonal_components$time.series[, "remainder"]
home_prices_imp
```

```{r}
# Plot the decomposed components
plot(decomposed$seasonal, main = "Seasonal Component")

plot(decomposed$trend, main = "Trend")
```

```{r}
# perform Augmented Dickey-Fuller test
library(tseries)

# convert the deseasonalized column to a numeric vector
home_prices_imp$deseasonalized <- as.numeric(home_prices_imp$deseasonalized)

adf_test_result <- adf.test(home_prices_imp$deseasonalized)
adf_test_result

# check p-value for stationarity
if (adf_test_result$p.value < 0.05) {
  cat("Time series is stationary.\n")
} else {
  cat("Time series is not stationary. Applying differencing.\n")
}

# apply differencing to the deseasonalized component
home_prices_imp$deseasonalized_diff <- c(NA, diff(home_prices_imp$deseasonalized))
home_prices_imp
```

```{r}
ACF <- acf(home_prices_imp$CA, lag.max = 50)
pacf <- pacf(home_prices_imp$CA, lag.max = 50)
```
```{r}
# remove missing values 
clean_diff<- na.omit(home_prices_imp$deseasonalized_diff)

# calculate ACF/PACF on cleaned df 
ACF_diff <- acf(clean_diff, lag.max = 50)
pacf_diff <- pacf(clean_diff, lag.max = 50)
```

```{r}
#simple moving average calculation
home_prices_imp$MA <- zoo::rollapply(home_prices_imp$CA, width = 4, FUN = mean, fill = NA, align = "right")

ggplot(home_prices_imp, aes(x = date, y = CA)) +
  geom_line(aes(y = CA, color = "Actual Sales")) +
  geom_line(aes(y = MA, color = "MA")) +
  labs(title = "", x = "Year", y = "")+
  scale_color_manual(values = c("Actual Sales" = "blue", "MA" = "red")) + 
  theme_minimal()
```
```{r}
home_prices_imp
```

```{r}
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
```

```{r}
# rename df 
CA <- home_prices_imp

CA
```

```{r}
# create a ts for CA column 
CA_ts <- ts(CA$CA, frequency = 12, start = c(1990, 1), end = c(2023, 9))
```

## Feature Enginering 

```{r}
# create additional lag features
CA$lag1 <- lag(CA$deseasonalized_diff, 1)
CA$lag2 <- lag(CA$deseasonalized_diff, 2)
CA$lag3 <- lag(CA$deseasonalized_diff, 3)

# rolling statistics
CA$rolling_std <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = sd, fill = NA, align = "right")
CA$rolling_mean <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = mean, fill = NA, align = "right")
```

```{r}
# calculate correlation matrix
cor_matrix <- cor(CA[, c("CA", "lag1", "lag2", "lag3", "rolling_std", "rolling_mean")], use = "complete.obs")

print(cor_matrix)
```

```{r}
# apply log transformation to the target variable - data transformation 
CA$CA_log <- log(CA$CA + 1)
```

## Train-Test Split

```{r}
# Create 2 Partitions

# Training set is from January 2014- December 2018
# Validation set is from January 2019 - September 2023
train <- window(CA_ts, start = c(2014, 1), end = c(2018, 12))
validation <- window(CA_ts, start = c(2019, 1))
```

```{r}
autoplot(train) + 
  autolayer(validation, color = "blue")
```

## Modeling

```{r}
# fit linear model
home_prices.lm <- tslm(CA_ts ~ trend + I(trend^2))

# plot linear model 
plot(CA_ts, xlab = "Time", ylab = "Shipments", ylim = c(160000 , 900000), bty = "l")
lines(home_prices.lm$fitted, lwd = 2)

CA_ts_zoom <- window(CA_ts, start = c(1990, 1), end = c(2023, 9))
plot(CA_ts_zoom, xlab = "Year", ylab = "CA", ylim = c(160000 , 900000), bty = "l")

```

```{r}
# Naive Model
n_model <- naive(train, h = 5, level = 95)
```

```{r}
# plot the Naive Model - Training set
autoplot(train) + 
  autolayer(n_model, color = "red", series = "Naive") +
   autolayer(validation, color = "blue", series = "Actual")

```

```{r}
# Seasonal Naive Model
sn_model <- snaive(train, h = 5)
```

```{r}
# plot the naive Model - Training set
autoplot(train) + 
  autolayer(sn_model, color = "red", series = "Seasonal Naive") +
   autolayer(validation, color = "blue", series = "Actual")

```

```{r}
# Season and Trend
st_model <- tslm(train ~ trend + season)

summary(st_model)
```

```{r}
st_forecast <- forecast(st_model, h=5)
st_forecast
```

```{r}
# plot the Season and Trend Model
autoplot(train) + 
  autolayer(st_forecast, color = "red", series = "season trend") +
   autolayer(validation, color = "blue", series = "Actual")

```

```{r}
# Determining the Error
accuracy(st_forecast)
accuracy(n_model)
accuracy(sn_model)
```

```{r}
mean(abs(n_model$residuals), na.rm = T) 
```

```{r}
sqrt(mean(n_model$residuals^2, na.rm = T)) 
```
```{r}
# Fit an ARIMA model using the training set
arima_model <- auto.arima(train)

# Print the model summary
print(summary(arima_model))

# Forecast future values using the ARIMA model on the validation set
forecast_values <- forecast(arima_model, h = length(validation))  

# Plot the original time series, fitted values, and forecast
autoplot(CA_ts) +
  autolayer(fitted(arima_model), series = "Fitted", color = "blue") +
  autolayer(forecast_values, series = "Forecast", color = "red") +
  labs(title = "ARIMA Model Fitting and Forecasting", y = "CA Sales") +
  theme_minimal()

# Evaluate the forecast accuracy on the validation set
accuracy(forecast_values, validation)

# Print the forecast values
print(forecast_values)

```
```{r}
# Fit an ETS model using the univariate time series
ets_model <- ets(train)

# Print the model summary
print(summary(ets_model))

# Forecast future values using the ETS model on the validation set
forecast_values <- forecast(ets_model, h = length(validation))  

# Plot the original time series, fitted values, and forecast
autoplot(CA_ts) +
  autolayer(fitted(ets_model), series = "Fitted", color = "blue") +
  autolayer(forecast_values, series = "Forecast", color = "red") +
  labs(title = "ETS Model Fitting and Forecasting", y = "CA Sales") +
  theme_minimal()

# Evaluate the forecast accuracy on the validation set
accuracy(forecast_values, validation)

# Print the forecast values
print(forecast_values)
```
```{r}
# create a ts for CA column 
riv_ts <- ts(CA$`San Diego`, frequency = 12, start = c(1990, 1), end = c(2023, 9))

# Create 2 Partitions

# Training set is from January 2014- December 2018
# Validation set is from January 2019 - September 2023
train_riv <- window(riv_ts, start = c(2014, 1), end = c(2018, 12))
validation_riv <- window(riv_ts, start = c(2019, 1))


# Fit an ETS model using the univariate time series
rivets_model <- ets(train_riv)

# Print the model summary
print(summary(rivets_model))

# Forecast future values using the ETS model on the validation set
forecast_values <- forecast(rivets_model, h = length(validation))  

# Plot the original time series, fitted values, and forecast
autoplot(riv_ts) +
  autolayer(fitted(rivets_model), series = "Fitted", color = "blue") +
  autolayer(forecast_values, series = "Forecast", color = "red") +
  labs(title = "ETS Model Fitting and Forecasting", y = "SM Sales") +
  theme_minimal()

# Evaluate the forecast accuracy on the validation set
accuracy(forecast_values, validation)

# Print the forecast values
print(forecast_values)
```

