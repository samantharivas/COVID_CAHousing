---
title: "COVID_CAHOUSING"
output: pdf_document
date: "2023-11-16"
---

# Effect of COVID on California Housing Prices (1990-2023)

Data Source: <https://www.car.org/marketdata/data/housingdata>.

The data was sources from the California Association of Realtor website.The data contains the median price of existing single-family detached homes in California, ranging from January 1990 until September 2023.

GitHub link: <https://github.com/samantharivas/COVID_CAHousing.git>

## Import Libraries

```{r}
# import necessary libaries 
library(tidyverse)
library(fpp3)
library(gridExtra)
library(ggplot2)
library(tsibble)
library(forecast)
library(patchwork)
library(fable)
library(readxl)
library(moments)
library(imputeTS)
library(zoo)
library(TTR)
```

## Importing Data

```{r}
# import excel file 
home_prices <- read_excel("MedianPricesofExistingDetachedHomesHistoricalData.xlsx", skip = 6) |>
  mutate(`Mon-Yr` = as.Date(`Mon-Yr`, format = "%Y-%m")) |>
  as_tsibble(index = "Mon-Yr") # create tsibble 

# plot ts object 
autoplot(home_prices, CA) + 
  theme_minimal() 
```

## Data Inspection

```{r}
# check dimension of df 
dim(home_prices) # df holds 405 rows and 65 columns 
home_prices
```

```{r}
str(home_prices) # df consists of numeric and character values 
```

```{r}
# convert columns to correct data type 

# specify the columns labeled as character to convert to numeric
cols_to_convert <- c("Amador", "Calaveras", "Contra-Costa", "Del Norte", "El Dorado", "Glenn", "Kings", "Lake", 
                   "Lassen", "Madera", "Mariposa", "Nevada", "Plumas", "San Benito", "San Mateo", 
                   "Shasta", "Siskiyou", "Solano", "Stanislaus", "Sutter", "Tehama", "Trinity", 
                   "Tuolumne", "Monterey", "Mono", "Santa Cruz", "San Luis Obispo", 
                   "San Benito", "San Mateo", "Shasta", "Siskiyou", "Solano", "Sonoma", "Stanislaus", 
                   "Sutter", "Tehama", "Trinity", "Tulare", "Ventura", "Yolo", "Yuba", "Condo", 
                   "LA Metro", "Central Coast", "Central Valley", "Far North", "Inland Empire", 
                   "S.F. Bay Area", "SoCal", "Mendocino", "San Joaquin")

# convert listed columns to numeric
for (col in cols_to_convert) {
  home_prices[[col]] <- as.numeric(as.character(home_prices[[col]]))
}

# check the structure of your data frame after conversion
str(home_prices)

```

```{r}
# convert date to MM/YYYY format
home_prices$date <- as.Date(home_prices$`Mon-Yr`, format = "%Y-%m-%d")
home_prices$date <- as.Date(home_prices$date, format ="%m %Y")

# create seperate column for month and year
home_prices <- home_prices %>%
  mutate(Month = month(date), Year = year(date))

# remove mon-yr column and ...55 column 
columns_to_keep <- setdiff(names(home_prices), c("Mon-Yr", "...55"))
home_prices <- home_prices[, columns_to_keep]

#view the updated data frame
print(home_prices)
```

```{r}
# make a subset df removing condo, LA metro, Central Coast, Central Valley, Far North, Inland Empire, and S.F. Bay Area 
regions_to_exclude <- c("Condo", "LA Metro", "Central Coast", "Central Valley", "Far North", "Inland Empire", "S.F. Bay Area", "SoCal")

home_prices <- home_prices[, !(colnames(home_prices) %in% regions_to_exclude)]
```

```{r}
#check for duplicates
sum(duplicated(home_prices)) # no duplicate values are present 
```

## Exploratory Data Analysis

```{r}
#identify outliers using boxplot
par(mfrow = c(2, 4))

# Exclude the "Mon-Yr" column
numeric_columns <- names(home_prices)[sapply(home_prices, is.numeric) & names(home_prices) != "Mon-Yr"]

for (col in numeric_columns) {
  boxplot(home_prices[[col]],
          xlab = col,
          main = paste(col, "Boxplot"))
}

for (col in numeric_columns) {
  hist(home_prices[[col]],
          xlab = col,
          main = paste(col, "Histogram"))
}

```

```{r}
# check for missing values 
missing_values <- colSums(is.na(home_prices))
print(missing_values)

# check missing percentage of values per column
missing_percentage <- colMeans(is.na(home_prices)) * 100
missing_percentage <- missing_percentage[order(-missing_percentage)]
missing_percentage
```

We decided to drop columns that data was more than 15% missing to maintain data integrity.

```{r}
# identify columns with more than 50% missing values
columns_to_drop <- names(missing_percentage[missing_percentage > 15])

# print columns with more than 15% missing values
cat("Columns with More than 15% Missing Values:\n")
cat(columns_to_drop, sep = ", ")

# drop the identified columns with more than 50% missing values from the DataFrame
home_prices <- home_prices[, !(names(home_prices) %in% columns_to_drop)]

```

```{r}
# use imputer to fill missing value with mean
home_prices_imp <- home_prices %>%
  mutate(across(-date, ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# exclude non-numeric columns 
numeric_columns_imp <- home_prices_imp[sapply(home_prices_imp, is.numeric)] 

# calculate skewness for all numeric imputed variables
skewness_values_imp <- moments::skewness(numeric_columns_imp, na.rm = TRUE)

print(skewness_values_imp)
```

```{r}
# calculate skewness of original df 
# exclude non-numeric columns 
numeric_columns <- home_prices[sapply(home_prices, is.numeric)] 

# calculate skewness for all numeric variables
skewness_values <- moments::skewness(numeric_columns, na.rm = TRUE)

print(skewness_values)
```

```{r}
# compare skewness before and after 
skewness_all <- data.frame(
  Original_Skewness = skewness_values,
  Skewness_Imputed= skewness_values_imp
)

skewness_all
```

```{r}
summary(home_prices_imp)
```

```{r}
dim(home_prices_imp) # new df has 405 rows and 34 columns 
str(home_prices_imp)
```

```{r}
# decompose ts 
# decompose the time series 
decomposed <- decompose(ts(home_prices_imp$CA, frequency = 12))

autoplot(decomposed) +
  labs(title = "Seasonal Decomposition")
```
```{r}
plot(home_prices_imp$CA, type = "l", main = "Time Series Plot")

# perform seasonal decomposition using STL
# convert df into ts 
ts_data <- ts(home_prices_imp, frequency = 12)
CA_ts <- ts(home_prices_imp$CA, frequency = 12)
seasonal_components <- stl(CA_ts, s.window = "periodic")

# deseasonalize the data
home_prices_imp$deseasonalized <- seasonal_components$time.series[, "remainder"]
home_prices_imp
```

```{r}
# Plot the decomposed components
plot(decomposed$seasonal, main = "Seasonal Component")

plot(decomposed$trend, main = "Trend")
```

```{r}
# perform Augmented Dickey-Fuller test
library(tseries)

# convert the deseasonalized column to a numeric vector
home_prices_imp$deseasonalized <- as.numeric(home_prices_imp$deseasonalized)

adf_test_result <- adf.test(home_prices_imp$deseasonalized)
adf_test_result

# check p-value for stationarity
if (adf_test_result$p.value < 0.05) {
  cat("Time series is stationary.\n")
} else {
  cat("Time series is not stationary. Applying differencing.\n")
}

# apply differencing to the deseasonalized component
home_prices_imp$deseasonalized_diff <- c(NA, diff(home_prices_imp$deseasonalized))
home_prices_imp
```

```{r}
ACF <- acf(home_prices_imp$CA, lag.max = 50)
pacf <- pacf(home_prices_imp$CA, lag.max = 50)
```
```{r}
# remove missing values 
clean_diff<- na.omit(home_prices_imp$deseasonalized_diff)

# calculate ACF/PACF on cleaned df 
ACF_diff <- acf(clean_diff, lag.max = 50)
pacf_diff <- pacf(clean_diff, lag.max = 50)
```

```{r}
#simple moving average calculation
home_prices_imp$MA <- zoo::rollapply(home_prices_imp$CA, width = 4, FUN = mean, fill = NA, align = "right")

ggplot(home_prices_imp, aes(x = date, y = CA)) +
  geom_line(aes(y = CA, color = "Actual Sales")) +
  geom_line(aes(y = MA, color = "MA")) +
  labs(title = "", x = "Year", y = "")+
  scale_color_manual(values = c("Actual Sales" = "blue", "MA" = "red")) + 
  theme_minimal()
```
```{r}
home_prices_imp
```

```{r}
correlation_matrix <- cor(ts_data, use = "pairwise.complete.obs")
correlation_matrix
```

```{r}
# rename df 
CA <- home_prices_imp

CA
```

```{r}
# create a ts for CA column 
CA_ts <- ts(CA$CA, frequency = 12, start = c(1990, 1), end = c(2023, 9))
```

## Feature Enginering 

```{r}
# create additional lag features
CA$lag1 <- lag(CA$deseasonalized_diff, 1)
CA$lag2 <- lag(CA$deseasonalized_diff, 2)
CA$lag3 <- lag(CA$deseasonalized_diff, 3)

# rolling statistics
CA$rolling_std <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = sd, fill = NA, align = "right")
CA$rolling_mean <- zoo::rollapply(CA$deseasonalized_diff, width = 3, FUN = mean, fill = NA, align = "right")
```

```{r}
# calculate correlation matrix
cor_matrix <- cor(CA[, c("CA", "lag1", "lag2", "lag3", "rolling_std", "rolling_mean")], use = "complete.obs")

print(cor_matrix)
```

```{r}
# apply log transformation to the target variable - data transformation 
CA$CA_log <- log(CA$CA + 1)
```

## Train-Test Split

```{r}
# Create 2 Partitions

# Training set is from January 2014- December 2018
# Validation set is from January 2019 - September 2023
train <- window(CA_ts, start = c(2014, 1), end = c(2018, 12))
validation <- window(CA_ts, start = c(2019, 1))
```

```{r}
autoplot(train) + 
  autolayer(validation, color = "blue")
```

## Modeling


```{r}
train
```


## Modeling


# Naive Model
```{r}
n_model <- naive(train, h = 57)
```

```{r}
# plot the Naive Model - Training set
autoplot(train) + 
  autolayer(n_model, color = "red", series = "Naive") +
   autolayer(validation, color = "blue", series = "Actual")

```

```{r}
summary(n_model)
```



# Seasonal Naive Model
```{r}
sn_model <- snaive(train, h = 57)
```

```{r}
# plot the naive Model - Training set
autoplot(train) + 
  autolayer(sn_model, color = "red", series = "Seasonal Naive") +
   autolayer(validation, color = "blue", series = "Actual")

```


# Season and Trend - Time Series Linear Model 

```{r}

st_model <- tslm(train ~ trend + season)

summary(st_model)
```

```{r}
st_forecast <- forecast(st_model, h=57)
st_forecast
```

```{r}
# plot the Season and Trend Model
autoplot(train) + 
  autolayer(st_forecast, color = "red", series = "season trend") +
   autolayer(validation, color = "blue", series = "Actual")

```

```{r}
# Determining the Error
accuracy(n_model)
accuracy(sn_model)
accuracy(st_forecast)
```



## SARIMA model

SARIMA models are used for time series data with clear seasonality and trends, and it includes components from seasonality, trend, and autoregression. It can capture complex seasonal patterns and trends in the data. It assumes that the future behavior of the time series depends linearly of its past values and past forecast errors. 

```{r}
# SARIMA model
sarima_model <- auto.arima(train)
sarima_forecast <- forecast(sarima_model, h = 57)

# plot SARIMA model
autoplot(train) + 
  autolayer(sarima_forecast, color = "green", series = "SARIMA Forecast") +
  autolayer(validation, color = "blue", series = "Actual")
```

## Theta Model

Suitable for time series with no clear patterns, trends, or seasonality. The theta model involves a smoothing parameter (theta) to adjust for random fluctuations. They work well for time series that show irregular or random behavior. It may not perform well for data with strong seasonality or trend. 

```{r}
# Fit the Theta model
theta_model <- thetaf(train)

# Generate forecasts
theta_forecast <- forecast(theta_model, h = 5)

# Plot the forecast
autoplot(theta_forecast)
```
## TBATS (Trigonometric, Box-Cox, ARMA errors, Trend and Seasonal) Model

TBATS is an extension of seasonal decomposition of time series (STL) method and it can capture multiple seasonalities, long-term trends, and autoregressive components. 

Trigonometric (T): Captures seasonality in the data using trigonometric functions. It's particularly useful when dealing with data that exhibits complex seasonal patterns.
Box-Cox (B): Applies a Box-Cox transformation to stabilize the variance of the time series. This is especially beneficial when the variance of the data changes over time.
ARMA Errors (A): Incorporates Autoregressive Moving Average (ARMA) components to model the autocorrelation structure in the residuals.
Trend (T): Models the long-term trend in the data, allowing TBATS to handle data with both short-term seasonality and long-term trends.
Seasonal (S): Represents the seasonal component of the time series. TBATS can handle multiple seasonal patterns, making it versatile for various types of data.

TBATS is a flexible model that can adapt to different time series patterns and is particularly useful for datasets with multiple seasonalities and complex structures.

```{r}
# Fit the TBATS model
tbats_model <- tbats(train)

# Generate forecasts
tbats_forecast <- forecast(tbats_model, h = 57)

# Plot the forecast
autoplot(tbats_forecast) +
  autolayer(validation, color = "blue", series = "Actual")
```

## Evaluation Metrics

```{r}
# Evaluate accuracy
sarima_accuracy <- accuracy(sarima_forecast)
theta_accuracy <- accuracy(theta_forecast)
tbats_accuracy <- accuracy(tbats_forecast)
naive_accuracy <- accuracy(n_model)
snaive_accuracy <- accuracy(sn_model)
st_accuracy <- accuracy(st_forecast)

# Create a table
accuracy_table <- data.frame(
  Model = c("SARIMA", "Theta", "TBATS", "Naive", "Seasonal_Naive", "Trend_Season"),
  RMSE = c(sarima_accuracy[1], theta_accuracy[1], tbats_accuracy[1], naive_accuracy[1], snaive_accuracy[1], st_accuracy[1]),
  MAE = c(sarima_accuracy[2], theta_accuracy[2], tbats_accuracy[2], naive_accuracy[2], snaive_accuracy[2], st_accuracy[2]),
  MAPE = c(sarima_accuracy[3], theta_accuracy[3], tbats_accuracy[3], naive_accuracy[3], snaive_accuracy[3], st_accuracy[3])
)


# Print the table
print(accuracy_table)

```